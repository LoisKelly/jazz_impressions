{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jazz Impressions: AI-Driven Visual Echoes of Musical Influences\n",
    "<p style=\"text-align:center;\">by Lois Kelly</p>\n",
    "\n",
    "----\n",
    "\n",
    "<p style=\"text-align:center;\">This code is submitted as in partial fulfilment for the requirements for the degree of </p>\n",
    "\n",
    "<p style=\"text-align:center;\">MASTER OF SCIENCE in Data Science and AI for the Creative Industries.  </p>\n",
    "\n",
    "<p style=\"text-align:center;\">Creative Computing Institue,  </p>\n",
    "\n",
    "<p style=\"text-align:center;\">University of the Arts Londond </p>\n",
    "\n",
    "<p style=\"text-align:center;\">2024. </p>\n",
    "\n",
    "----\n",
    "\n",
    "- To run this notebook please ensure the full github repo has been downloaded: https://git.arts.ac.uk/23044972/Jazz_Impressions/tree/main\n",
    "\n",
    "- This code requires access to a GPU set up using CUDA. \n",
    "\n",
    "- *LLM DISCLAIMER: This code was developed with assistance from ChatGTP. Other sources are cited at the bottom of the relevent cell.*\n",
    "\n",
    "----\n",
    "## Contents\n",
    "\n",
    "1. [Imports and Configuration](#imports-and-configuration)\n",
    "2. [Chunk Processing](#chunk-processing)\n",
    "3. [Main Function](#main-function)\n",
    "4. [Adding the Audio File](#adding-the-audio-file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and configuration\n",
    "\n",
    "To begin, load the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from os import mkdir\n",
    "from os.path import join\n",
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import torchaudio\n",
    "from skimage import img_as_ubyte\n",
    "from visualiserfunc_ import generate_interpolated_images\n",
    "import sdtransitions_\n",
    "from sdtransitions_ import generate_frames\n",
    "from tqdm import tqdm  # For progress bar\n",
    "from datetime import datetime\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from jazzclassification import predict\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the solo file that is being used into the ``` audio_path``` variable. The configuration can be adjusted for experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "audio_path = 'exampleaudio.wav' # Add the solo here \n",
    "instruments = r'instruments.csv' \n",
    "chunk_duration = 5  # Length of audio chunks the solo will split into\n",
    "fps = 20 # Number of frames per second in the final output video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sets up the functions to process the audio, ready for classificaiton as well as setting up the instrument function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing functions\n",
    "def split_audio(audio_path, chunk_duration=5):\n",
    "    try:\n",
    "        waveform, sample_rate = librosa.load(audio_path, sr=None)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio file: {e}\")\n",
    "        return []\n",
    "    \n",
    "    # Calculate the number of samples per chunk\n",
    "    chunk_size = int(sample_rate * chunk_duration)\n",
    "\n",
    "    # Create output folder\n",
    "    output_dir = \"chunks\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    filenames = []\n",
    "    \n",
    "    # Split the audio file into 5 seconds chunks \n",
    "    for i, start in enumerate(range(0, len(waveform), chunk_size)):\n",
    "        end = min(start + chunk_size, len(waveform))\n",
    "        chunk = waveform[start:end]\n",
    "        chunk_file = os.path.join(output_dir, f\"chunk_{i+1}.wav\")\n",
    "        \n",
    "        # Save chunk\n",
    "        sf.write(chunk_file, chunk, sample_rate)\n",
    "        filenames.append(chunk_file)\n",
    "        print(f\"Saved chunk {i+1} to {chunk_file}\")\n",
    "    \n",
    "    return filenames\n",
    "\n",
    "# Working from the instruments.csv file. Ensure this is within the same folder.\n",
    "def find_instrument(artist_name):\n",
    "    with open(instruments, mode='r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            if row[0] == artist_name:\n",
    "                return row[1]  \n",
    "    return None  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk Processing \n",
    "\n",
    "Here the functions to process each chunk of audio are set up. It includes making the prediction and then setting up the stable diffusion prompts with the relevent predicitons and instruments. Then the correct LoRA weights are loaded in and the frames are generated for both the blended images and the transition images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(filename):\n",
    "    waveform, sample_rate = torchaudio.load(filename)\n",
    "    new_sample_rate = 8000\n",
    "    transform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=new_sample_rate)\n",
    "    waveform = transform(waveform)\n",
    "    \n",
    "    predictions = predict(waveform)  # You would need to define `predict`\n",
    "\n",
    "    # Unpack top 2 predictions\n",
    "    prediction_1_label, weight_1 = predictions[0]\n",
    "    print(prediction_1_label)\n",
    "    prediction_2_label, weight_2 = predictions[1]\n",
    "    # Find instruments for each prediction\n",
    "    instrument_1 = find_instrument(prediction_1_label)\n",
    "    instrument_2 = find_instrument(prediction_2_label)\n",
    "\n",
    "    # Create prompts for image generation\n",
    "    prompt_1 = f'{prediction_1_label} playing the {instrument_1} in a jazzy  painting style with vivid colors and distinctive brush strokes. The images are bright and lively, with a focus on the artist and their instrument.'\n",
    "    print(prompt_1)\n",
    "    prompt_2 = f'{prediction_2_label} playing the {instrument_2} in a jazzy  painting style with vivid colors and distinctive brush strokes. The images are bright and lively, with a focus on the artist and their instrument.'\n",
    "\n",
    "    # Generate images and frames\n",
    "    model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "    lora1_path = f'weights/{prediction_1_label}_lora_weights.safetensors'\n",
    "    lora2_path = f'weights/{prediction_2_label}_lora_weights.safetensors'\n",
    "    adapter_names = [\"adapter1\", \"adapter2\"]\n",
    "    prompts = [prompt_1, prompt_2]\n",
    "\n",
    "    output_dir, image_paths = generate_interpolated_images(\n",
    "        model_id, adapter_names, prompts, lora1_path, lora2_path)\n",
    "    frames = generate_frames(output_dir, lora1_path, lora2_path, model_id, steps_per_image=20)\n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This functions brings all the frames for each processed chunk and puts them together into a video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def frames_to_video(frames, output_path, fps=30):\n",
    "    if not frames:\n",
    "        raise ValueError(\"No frames to write to video.\")\n",
    "    frame = np.array(frames[0])\n",
    "    height, width, layers = frame.shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    for frame in frames:\n",
    "        frame = np.array(frame)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        video.write(frame)\n",
    "    video.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function\n",
    "\n",
    "Here all of the functions are brought together to create the visualiser video. An output directory is set up using a current time stamp. Then a for loop processes each 5 second chunk in order and the frames are saved to the ```all_frames``` array. The frames are then put together to create the video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    timestampStr = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = join('output_videos', timestampStr)\n",
    "     \n",
    "    # This creates an output directory that is labelled with the timestamp\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    filenames = split_audio(audio_path, chunk_duration)\n",
    "    \n",
    "    all_frames = []\n",
    "    for i, chunk_file in tqdm(enumerate(filenames), desc=\"Processing chunks\", unit=\"chunk\"):\n",
    "        frames = process_chunk(chunk_file)\n",
    "        all_frames.extend(frames)   \n",
    "\n",
    "    output_path = os.path.join(output_dir, 'output_video.mp4')\n",
    "    frames_to_video(all_frames, output_path, fps=fps)\n",
    "    print(f\"Video saved at {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the Audio File\n",
    "\n",
    "Finally, the audio file is added to the visualisation video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestampStr = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_path = join('fullvideo', timestampStr)\n",
    "output_final = os.path.join(output_path, 'full.mp4')\n",
    "\n",
    "\n",
    "subprocess.run([\n",
    "    \"ffmpeg\", \"-i\", output_path, \"-i\", audio_path, \n",
    "    \"-c:v\", \"copy\", \"-c:a\", \"aac\", output_final\n",
    "])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesisproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
